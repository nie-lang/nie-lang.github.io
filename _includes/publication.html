<head>
	<style>
		body{
			color: #333;
		}
		#container{
			min-width: 1000px;
			width: 1000px;
/*			overflow: auto;
*/			margin: 50px auto;padding: 30px;
			/*zoom: 1;*/
*//*			border: 1px solid #ccc;background: #fc9;color: #fff;
*/		}
		#left{
			float: left;
			width: 400px;
			height: 230px;
			margin-left: 0px;
		}
		#right{
			float: left;
			width: auto;
			margin-left: 50px;
		}
		#name{
			font-size: 22.0pt;
		    mso-bidi-font-size: 24.0pt;
		    font-family: Times;
		    mso-bidi-font-family: Times;
		        font-weight: bold;
		}
		#info{
		    font-size: 16.0pt;
		    mso-bidi-font-size: 17.0pt;
		    font-family: Times;
		    mso-bidi-font-family: Times;
		    margin-top: 30px;
		    margin-left: 5px;
		    margin-bottom: 10px;
		    
		}
		.clear{clear:both; height: 0; line-height: 0; font-size: 0}
		.Bio{
			font-size:16.0pt;
			mso-bidi-font-size:17.0pt;
			line-height:150%;
			font-family:Times;
			mso-bidi-font-family:Lato-Regular;
			text-align: justify;
		}
		span.SpellE {
		    mso-style-name: "";
		    mso-spl-e: yes;
		}
		span.Title{
			    font-size: 22.0pt;
			    mso-bidi-font-size: 17.0pt;
			    font-family: Times;
			    mso-bidi-font-family: Lato-Regular;
			    font: bold;
			    margin-top: 10px;
		}
		div.section{
			padding-top: 30px;
		}
		
		div.sub-left{
			float: left;
			width: 250px;
						
		}
		div.sub-left img{
			vertical-align: middle;
			horizontal-align: middle;
			margin-top: 10px;
		}
		
		div.sub-left span{
			height: 100%;
			display: inline-block;
			vertical-align: top;
						
		}
		div.sub-right{
			float: left;
			width: 800px;			
		}
		.paper{
			overflow: auto;
			zoom:1;
			padding-bottom: 0px;
			min-height: 150px;

		}
		.paperTitle{
			font-size:14.0pt;
			mso-bidi-font-size:18.0pt;
			font-family:Times;
			mso-bidi-font-family:Times;
			margin-top: 10px;
			margin-bottom: 10px;
			font-weight: bold;
		}
		.paperName,.paperPub{
		    font-size: 12.0pt;
		    mso-bidi-font-size: 13.0pt;		    
		    font-family: Times;
		    mso-bidi-font-family: Times;
		    line-height:150%;
		}
		.link{
		    font-size: 12.0pt;
		    mso-bidi-font-size: 13.0pt;
		    font-family: Times;
		    mso-bidi-font-family: Times;
		    margin-top: 10px;
		    margin-bottom: 0px;
		}
		.special{
		    margin-top: 0in;
		    margin-bottom: 0in;
		    margin-left: -.9pt;
		    margin-bottom: .0001pt;
		    text-indent: .9pt;
		    mso-pagination: none;
		    tab-stops: 13.75in;
		    mso-layout-grid-align: none;
		    text-autospace: none;
		}
		.long div.sub-left, .long div.sub-right{
			height: 300px;
			width: 1200;

		}
		.short div.sub-left, .short div.sub-right{
			height:160px;

		}
		div.sub-left,div.sub-right{
			height:200px;

		}
	</style>
</head>


<h3>
	<a name='publications'></a> Selected Publications
</h3>

<h4>
	<a name='pre'></a> <b>Preprint</b>
</h4>

		<div class="paper short">
				<div class="sub-left">
					<span></span>
					<img src="assets/images/rotation.png" width="240" height="130">
				</div>
				<div class="sub-right">
				<div class="media">
				  <div class="media-body">
					<p class="media-heading">
					  <strong>Deep Rotation Correction without Angle Prior</strong><br />
					   <strong>Lang Nie</strong>, Chunyu Lin, Kang Liao, Shuaicheng Liu, Yao Zhao<br />
					   <a href="https://arxiv.org/abs/2207.03054">[Paper]</a>
					   <a href="https://github.com/nie-lang/RotationCorrection">[Code]</a>
						<a href="https://github.com/nie-lang/RotationCorrection">[Dataset]</a>
					</p>
				  </div>
				</div>	
				</div>
			</div>

			<div class="paper short">
				<div class="sub-left">
					<span></span>
					<img src="assets/images/RecRecNet.png" width="240" height="130">
				</div>
				<div class="sub-right">
				<div class="media">
				  <div class="media-body">
					<p class="media-heading">
					  <strong>RecRecNet: Rectangling Rectified Wide-Angle Images by Thin-Plate Spline Model and DoF-based Curriculum Learning</strong><br />
					   Kang Liao*, <strong>Lang Nie*</strong>, Chunyu Lin, Zishuo Zheng, Yao Zhao<br />
					   (* Equal contribution) <br />
					   <a href="https://arxiv.org/abs/2301.01661">[Paper]</a>
					   <a href="">[Code]</a>
					</p>
				  </div>
				</div>	
				</div>
			</div>

<h4>
	<a name='2023'></a> <b>2023</b>
</h4>
			<div class="paper short">
				<div class="sub-left">
					<span></span>
					<img src="assets/images/TCSVT23-ADAP.png" width="240" height="130">
				</div>
				<div class="sub-right">
				<div class="media">
				  <div class="media-body">
					<p class="media-heading">
					  <strong>SivsFormer: Parallax-Aware Transformers for Single-image-based View Synthesis</strong><br />
					   Chunlan Zhang, Chunyu Lin, Kang Liao, <strong>Lang Nie</strong>, Yao Zhao<br />
					   IEEE Transactions on Circuits and Systems for Video Technology (<strong>TCSVT</strong>)<br />
					   <a href="https://ieeexplore.ieee.org/abstract/document/10019290">[Paper]</a>
					</p>
				  </div>
				</div>	
				</div>
			</div>

			<div class="paper short">
				<div class="sub-left">
					<span></span>
					<img src="assets/images/pano_seg.png" width="240" height="130">
				</div>
				<div class="sub-right">
				<div class="media">
				  <div class="media-body">
					<p class="media-heading">
					  <strong>Complementary Bi-directional Feature Compression for Indoor 360째 Semantic Segmentation with Self-distillation</strong><br />
					   Zishuo Zheng, Chunyu Lin, <strong>Lang Nie</strong>, Kang Liao, Zhijie Shen, Yao Zhao<br />
					   Winter Conference on Applications of Computer Vision (<strong>WACV</strong>)<br />
					   <a href="https://openaccess.thecvf.com/content/WACV2023/papers/Zheng_Complementary_Bi-Directional_Feature_Compression_for_Indoor_360deg_Semantic_Segmentation_With_WACV_2023_paper.pdf">[Paper]</a>
					</p>
				  </div>
				</div>	
				</div>
			</div>


<h4>
	<a name='2022'></a> <b>2022</b>
</h4>
		<div class="paper short">
				<div class="sub-left">
					<span></span>
					<img src="assets/images/CVPR22-Rectangling.png" width="240" height="130">
				</div>
				<div class="sub-right">
				<div class="media">
				  <div class="media-body">
					<p class="media-heading">
					  <strong>Deep Rectangling for Image Stitching: A Learning Baseline</strong><br />
					   <strong>Lang Nie</strong>, Chunyu Lin, Kang Liao, Shuaicheng Liu, Yao Zhao<br />
					   IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>). (<strong><font color="red">Oral presentation</font></strong>)<br/>
					   <a href="https://arxiv.org/abs/2203.03831">[Paper]</a>
					   <a href="https://github.com/nie-lang/DeepRectangling">[Code]</a>
					   <a href="https://github.com/nie-lang/DeepRectangling">[Dataset]</a>
					   <a href="https://www.youtube.com/watch?v=zx886swXZ2w&t=9s">[Video]</a>
					   <a href="https://zhuanlan.zhihu.com/p/492174095">[Chinese Blog]</a>
						
					</p>
				  </div>
				</div>	
				</div>
			</div>

		<div class="paper short">
				<div class="sub-left">
					<span></span>
					<img src="assets/images/panoformer.png" width="240" height="130">
				</div>
				<div class="sub-right">
				<div class="media">
				  <div class="media-body">
					<p class="media-heading">
					  <strong>PanoFormer: Panorama Transformer for Indoor 360째 Depth Estimation</strong><br />
					   Zhijie Shen, Chunyu Lin, Kang Liao, <strong>Lang Nie</strong>, Zishuo Zheng, Yao Zhao<br />
					   European Conference on Computer Vision (<strong>ECCV</strong>)<br />
					   <a href="https://arxiv.org/pdf/2203.09283.pdf">[Paper]</a>
					   <a href="https://github.com/zhijieshen-bjtu/PanoFormer">[Code]</a>
					</p>
				  </div>
				</div>	
				</div>
			</div>

		<div class="paper short">
				<div class="sub-left">
					<span></span>
					<img src="assets/images/VR22-Sivs.png" width="240" height="130">
				</div>
				<div class="sub-right">
				<div class="media">
				  <div class="media-body">
					<p class="media-heading">
					  <strong>SivsFormer: Parallax-Aware Transformers for Single-image-based View Synthesis</strong><br />
					   Chunlan Zhang, Chunyu Lin, Kang Liao, <strong>Lang Nie</strong>, Yao Zhao<br />
					   IEEE Conference on Virtual Reality and 3D User Interfaces (<strong>IEEE VR</strong>)<br />
					   <a href="https://ieeexplore.ieee.org/document/9756742">[Paper]</a>
					</p>
				  </div>
				</div>	
				</div>
			</div>

			<div class="paper short">
				<div class="sub-left">
					<span></span>
					<img src="assets/images/TCSVT22-contourlet.png" width="240" height="130">
				</div>
				<div class="sub-right">
				<div class="media">
				  <div class="media-body">
					<p class="media-heading">
					  <strong>Neural Contourlet Network for Monocular 360째 Depth Estimation</strong><br />
					   Zhijie Shen, Chunyu Lin, <strong>Lang Nie</strong>, Kang Liao, Yao Zhao<br />
					   IEEE Transactions on Circuits and Systems for Video Technology (<strong>TCSVT</strong>)<br />
					   <a href="https://ieeexplore.ieee.org/document/9833523">[Paper]</a>
					   <a href="https://github.com/zhijieshen-bjtu/Neural-Contourlet-Network-for-MODE">[Code]</a>
					</p>
				  </div>
				</div>	
				</div>
			</div>

			<div class="paper short">
				<div class="sub-left">
					<span></span>
					<img src="assets/images/pano_detection.jpg" width="240" height="130">
				</div>
				<div class="sub-right">
				<div class="media">
				  <div class="media-body">
					<p class="media-heading">
					  <strong>Bi-Projection for 360째 Image Object Detection Bridged by Roi Searcher</strong><br />
					   Zishuo Zheng, Chunyu Lin, <strong>Lang Nie</strong>, Kang Liao, Yao Zhao<br />
					   Journal of Visual Communication and Image Representation (<strong>JVCIR</strong>)<br />
					   <a href="https://www.sciencedirect.com/science/article/pii/S1047320322001808">[Paper]</a>
					</p>
				  </div>
				</div>	
				</div>
			</div>



<h4>
	<a name='2021'></a> <b>2021</b>
</h4>

		<div class="paper short">
				<div class="sub-left">
					<span></span>
					<img src="assets/images/TIP21-unsupervised.png" width="240" height="130">
				</div>
				<div class="sub-right">
				<div class="media">
				  <div class="media-body">
					<p class="media-heading">
					  <strong>Unsupervised Deep Image Stitching: Reconstructing Stitched Features to Images</strong><br />
					  <strong>Lang Nie</strong>, Chunyu Lin, Kang Liao, Shuaicheng Liu, Yao Zhao<br />
					   IEEE Transactions on Image Processing (<strong>IEEE TIP</strong>)<br />
					   <a href="https://arxiv.org/abs/2106.12859">[Paper]</a>
					   <a href="https://github.com/nie-lang/UnsupervisedDeepImageStitching">[Code]</a>
					   <a href="https://github.com/nie-lang/UnsupervisedDeepImageStitching">[Dataset]</a>
					   <a href="https://zhuanlan.zhihu.com/p/386863945">[Chinese Blog]</a>
					</p>
				  </div>
				</div>	
				</div>
			</div>

		<div class="paper short">
				<div class="sub-left">
					<span></span>
					<img src="assets/images/TCSVT21-homo.png" width="240" height="130">
				</div>
				<div class="sub-right">
				<div class="media">
				  <div class="media-body">
					<p class="media-heading">
					  <strong>Depth-Aware Multi-Grid Deep Homography Estimation with Contextual Correlation</strong><br />
					  <strong>Lang Nie</strong>, Chunyu Lin, Kang Liao, Shuaicheng Liu, Yao Zhao<br />
					   IEEE Transactions on Circuits and Systems for Video Technology (<strong>IEEE TCSVT</strong>)<br />
					   <a href="https://arxiv.org/abs/2107.02524">[Paper]</a>
					   <a href="https://github.com/nie-lang/Multi-Grid-Deep-Homography">[Code]</a>
					</p>
				  </div>
				</div>	
				</div>
			</div>


		<div class="paper short">
				<div class="sub-left">
					<span></span>
					<img src="assets/images/Neuro21-stitchnet2.png" width="240" height="130">
				</div>
				<div class="sub-right">
				<div class="media">
				  <div class="media-body">
					<p class="media-heading">
					  <strong>Learning Edge-Preserved Image Stitching from Multi-Scale Deep Homography</strong><br />
					  <strong>Lang Nie</strong>, Chunyu Lin, Kang Liao, Yao Zhao<br />
					   Neurocomputing (<strong>NEUCOM</strong>)<br />
					   <a href="https://www.sciencedirect.com/science/article/pii/S0925231221018701?casa_token=ljSQgciO6i8AAAAA:KCCzEx2-_N3GODg9-MsMzoHY-I8r0Zmdf8ksLQGeXEYY9VtfHaxPXzq_kFxuRn6Ap350dZ0vtg">[Paper]</a>
					</p>
				  </div>
				</div>	
				</div>
			</div>

		<div class="paper short">
				<div class="sub-left">
					<span></span>
					<img src="assets/images/ICME21-dualcube.jpg" width="240" height="130">
				</div>
				<div class="sub-right">
				<div class="media">
				  <div class="media-body">
					<p class="media-heading">
					  <strong>Distortion-Tolerant Monocular Depth Estimation on Omnidirectional Image using Dual-Cubemap</strong><br />
					  Zhijie Shen, Chunyu Lin, <strong>Lang Nie</strong>, Kang Liao, Yao Zhao<br />
					   IEEE International Conference on Multimedia and Expo (<strong>ICME</strong>)<br />
					   <a href="https://arxiv.org/abs/2203.09733">[Paper]</a>
					</p>
				  </div>
				</div>	
				</div>
			</div>


<h4>
	<a name='2020'></a> <b>2020</b>
</h4>

		<div class="paper short">
				<div class="sub-left">
					<span></span>
					<img src="assets/images/JVCIR20-view-free.png" width="240" height="130">
				</div>
				<div class="sub-right">
				<div class="media">
				  <div class="media-body">
					<p class="media-heading">
					  <strong>A View-Free Image Stitching Network Based on Global Homography</strong><br />
					  <strong>Lang Nie</strong>, Chunyu Lin, Kang Liao, Meiqin Liu, Yao Zhao<br />
					   Journal of Visual Communication and Image Representation (<strong>JVCIR</strong>)<br />
					   <a href="https://www.sciencedirect.com/science/article/pii/S1047320320301784?casa_token=d1t3NBEDZWQAAAAA:Vn8KVmkW_Bz7RjRD_EmFNzux2KZ4FVUx0wsCIRR88ePqT2jLYp4h9ZTGjByqu_xBEe-O6FYRKw">[Paper]</a>
					   <a href="https://github.com/nie-lang/DeepImageStitching-1.0">[Code]</a>
					</p>
				  </div>
				</div>	
				</div>
			</div>


